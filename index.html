<!DOCTYPE html>
<html lang="id">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Kumpulan Tugas</title>
  <link rel="stylesheet" href="style.css">

  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet"/>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

  <style>
    .content { display: none; }
    .content.active { display: block; }
  </style>
</head>
<body>
  <!-- Sidebar -->
  <div class="sidebar">
    <img src="profil.JPG" alt="Foto Profil">
    <h2>Kevin Agung J Mahendra</h2>
    <h2>200411100085</h2>
    <ul>
      <li><a href="#" onclick="showContent('tugas2')">Crawling PTA</a></li>
      <li><a href="#" onclick="showContent('tugas3')">Crawling Berita Online</a></li>
      <li><a href="#" onclick="showContent('tugas4')">Preprocessing PTA</a></li>
      <li><a href="#" onclick="showContent('tugas5')">Preprocessing Berita Online</a></li>
      <li><a href="#" onclick="showContent('tugas6')">TF-IDF & Word Embedding</a></li>
      <li><a href="#" onclick="showContent('tugas7')">LDA</a></li>
      <li><a href="#" onclick="showContent('uts1')">UTS 1</a></li>
      <li><a href="#" onclick="showContent('uts2')">UTS 2</a></li>
      <li><a href="#" onclick="showContent('tugas8')">Web Usage Mining</a></li>
      <li><a href="#" onclick="showContent('tugas9')">Tabel Web Usage Mining</a></li>
      <li><a href="#" onclick="showContent('tugas11')">Graph Facebook</a></li>
      <li><a href="#" onclick="showContent('tugas12')">Crawling X/Twitter</a></li>
      <li><a href="#" onclick="showContent('tugas10')">Revisi TF-IDF & Word Embedding</a></li>
      <li><a href="#" onclick="showContent('tugas11')">Wordh Graph</a></li>
      <li><a href="#" onclick="showContent('tugas12')">Load Paper</a></li>
    </ul>
  </div>

  <!-- Main Content -->
  <div class="main">
    <!-- Tugas 2 -->
    <div id="tugas2" class="content active">
      <h1>Crawling PTA</h1>
      <div class="code-box">
<pre><code class="language-python">
import requests as req
from bs4 import BeautifulSoup as bs
import csv
import pandas as pd
hades = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'}
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
def scrape_TA(page):
  global hades
  data=[]
  for p in range(1,page+1):
    if p==1 :
      URL = "https://pta.trunojoyo.ac.id/c_search/byprod/10/"
    else :
      URL = f"https://pta.trunojoyo.ac.id/c_search/byprod/10/{p}"
    request = req.get(URL,hades).text
    soup = bs(request, 'lxml')
    prodi = soup.find_all('div',{'id':'begin'})
    for pro in prodi:
      prod = pro.find('h2').text
    jur = prod[-18:]
    ul = soup.find('ul', 'items list_style')
    li = ul.find_all('li', {'data-id':'id-1'})
    for x in li:
      link = x.find('a','gray button')['href']
      request2 = req.get(link, hades).text
      soup2 = bs(request2, 'lxml')
      abst= soup2.find('p',{'align':'justify'}).text.replace('\r','').replace('\n','')
      main_div = soup2.find('div', style=lambda s: s and 'margin: 15px' in s)
      abstrak_id = ""
      abstrak_en = ""
      if main_div:
          b_tags = main_div.find_all('b')
          for b in b_tags:
              text = b.text.strip().lower()
              if "abstraksi" in text:
                  p_tag = b.find_next('p')
                  abstrak_id = p_tag.text.strip() if p_tag else ""
              elif "abstraction" in text:
                  p_tag = b.find_next('p')
                  abstrak_en = p_tag.text.strip() if p_tag else ""
      NPM = x.find('a','gray button')['href'][-12:]
      headline = x.find('a', 'title').text.replace('\r','').replace('\n','')
      pembimbing_pertama = x.select_one('span:contains("Dosen Pembimbing I")').text.split(' : ')[1]
      pembimbing_kedua = x.select_one('span:contains("Dosen Pembimbing II")').text.split(' :')[1]
      data.append([NPM,pembimbing_pertama,pembimbing_kedua,headline,abstrak_id,abstrak_en,jur])
      print([NPM,pembimbing_pertama,pembimbing_kedua,headline,abstrak_id,abstrak_en,jur])

  return data
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
data_TA=scrape_TA(2)
data_TA_result=pd.DataFrame(data_TA)
data_TA_result.columns = ["NPM","Pembimbing Pertama","Pembimbing Kedua","Judul","Abstrak","Abstraction","jurusan"]
data_TA_result
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
data_TA_result.to_csv('data_scrap_PTA.csv')
data_TA_result.to_excel('data_scrap_PTA.xlsx', index=False)
</code></pre>
      </div>
    </div>

    <!-- Tugas 3 -->
    <div id="tugas3" class="content">
      <h1>Crawling Berita Online</h1>
      <div class="code-box">
<pre><code class="language-python">
import requests as req
from bs4 import BeautifulSoup as bs
import csv
import pandas as pd
hades = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
from bs4 import BeautifulSoup as bs
import requests as req

def scrape_detik(page):
    global hades  # header atau session request
    data = []

    for p in range(1, page + 1):
        print(f"Scraping page {p} ...")
        url = f"https://www.detik.com/search/searchall?query=puan+maharani&siteid=2&sortby=time&page={p}"
        request = req.get(url, hades).text

        soup = bs(request, 'lxml')
        news_list = soup.find('div', class_='list-content')

        if news_list is None:
            print(f"Page {p}: div list-content tidak ditemukan")
            continue

        news_article = news_list.find_all('article')
        for x in news_article:
            # ambil judul
            title_tag = x.find('h3', class_='media__title')
            title = title_tag.text.strip() if title_tag else ""

            # ambil deskripsi
            desc_tag = x.find('div', class_='media__desc')
            desc = desc_tag.text.strip() if desc_tag else ""

            data.append([title, desc])

    return data

</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
news=scrape_detik(50)
news_result=pd.DataFrame(news)
news_result.columns = ["Judul", "Konten"]
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
news_result.to_csv('berita.csv',index=False)
news_result.to_excel('berita.xlsx', index=False)
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
import pandas as pd
#read in the data using pandas
url = pd.read_csv('/content/berita.csv')
url.head(50)
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
data_TA_result.to_csv('data_scrap_PTA.csv')
data_TA_result.to_excel('data_scrap_PTA.xlsx', index=False)
</code></pre>
      </div>

    </div>

    <!-- Tugas 4 -->
    <div id="tugas4" class="content">
      <h1>Preprocessing PTA</h1>
      <div class="code-box">
<pre><code class="language-python">
import requests as req
from bs4 import BeautifulSoup as bs
import csv
import pandas as pd
hades = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'}
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
def scrape_TA(page):
  global hades
  data=[]
  for p in range(1,page+1):
    if p==1 :
      URL = "https://pta.trunojoyo.ac.id/c_search/byprod/10/"
    else :
      URL = f"https://pta.trunojoyo.ac.id/c_search/byprod/10/{p}"
    request = req.get(URL,hades).text
    #var bs yang menyimpan data request berupa html
    soup = bs(request, 'lxml')
    prodi = soup.find_all('div',{'id':'begin'})
    for pro in prodi:
      prod = pro.find('h2').text
    jur = prod[-18:]
    ul = soup.find('ul', 'items list_style')
    li = ul.find_all('li', {'data-id':'id-1'})
    for x in li:
      link = x.find('a','gray button')['href']
      request2 = req.get(link, hades).text
      soup2 = bs(request2, 'lxml')
      abst= soup2.find('p',{'align':'justify'}).text.replace('\r','').replace('\n','')
      # ambil abstrak ID dan EN
      main_div = soup2.find('div', style=lambda s: s and 'margin: 15px' in s)
      abstrak_id = ""
      abstrak_en = ""
      if main_div:
          b_tags = main_div.find_all('b')
          for b in b_tags:
              text = b.text.strip().lower()
              if "abstraksi" in text:
                  p_tag = b.find_next('p')
                  abstrak_id = p_tag.text.strip() if p_tag else ""
              elif "abstraction" in text:
                  p_tag = b.find_next('p')
                  abstrak_en = p_tag.text.strip() if p_tag else ""
      NPM = x.find('a','gray button')['href'][-12:]
      headline = x.find('a', 'title').text.replace('\r','').replace('\n','')
      pembimbing_pertama = x.select_one('span:contains("Dosen Pembimbing I")').text.split(' : ')[1]
      pembimbing_kedua = x.select_one('span:contains("Dosen Pembimbing II")').text.split(' :')[1]
      data.append([NPM,pembimbing_pertama,pembimbing_kedua,headline,abstrak_id,abstrak_en,jur])
      print([NPM,pembimbing_pertama,pembimbing_kedua,headline,abstrak_id,abstrak_en,jur])

  return data
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
data_TA=scrape_TA(2)
# data_TA=scrape_TA(172)
data_TA_result=pd.DataFrame(data_TA)
data_TA_result.columns = ["NPM","Pembimbing Pertama","Pembimbing Kedua","Judul","Abstrak","Abstraction","jurusan"]
data_TA_result
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
!pip install Sastrawi
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
import requests as req
from bs4 import BeautifulSoup as bs
import pandas as pd
import re, string

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('punkt_tab')


from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
factory = StemmerFactory()
stemmer = factory.create_stemmer()
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
import re
import string
from bs4 import BeautifulSoup
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from nltk.tokenize import word_tokenize

# Buat stemmer & stopword remover
stemmer = StemmerFactory().create_stemmer()
stopword = StopWordRemoverFactory().create_stop_word_remover()

# Fungsi cleaning
def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()  # huruf kecil
    text = re.sub(r'\d+', '', text)  # hapus angka
    text = text.translate(str.maketrans('', '', string.punctuation))  # hapus tanda baca
    text = re.sub(r'\s+', ' ', text)  # rapikan spasi
    text = BeautifulSoup(text, "html.parser").get_text()  # hapus tag HTML
    return text.strip()

# 1. Cleaning abstrak
data_TA_result["clean_text"] = data_TA_result["Abstrak"].apply(clean_text)

# 2. Hapus stopword
data_TA_result["no_stopword"] = data_TA_result["clean_text"].apply(lambda x: stopword.remove(x))

# 3. Stemming (langsung kalimat penuh)
data_TA_result["Abstrak_Stem"] = data_TA_result["no_stopword"].apply(lambda x: stemmer.stem(x))

# 4. Tokenisasi setelah stemming
data_TA_result["Abstrak_Token"] = data_TA_result["Abstrak_Stem"].apply(lambda x: word_tokenize(x))

# Tampilkan hasil
print(data_TA_result[["Judul","Abstrak","clean_text","no_stopword","Abstrak_Stem","Abstrak_Token"]].head())
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
data_TA_result.to_csv('hasil_preprocessing.csv')
data_TA_result.to_excel('hasil_preprocessing.xlsx', index=False)
</code></pre>
      </div>
    </div>

    <!-- Tugas 5 -->
    <div id="tugas5" class="content">
      <h1>Preprocessing Berita Online</h1>
      <div class="code-box">
<pre><code class="language-python">
import requests as req
from bs4 import BeautifulSoup as bs
import csv
import pandas as pd
hades = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
from bs4 import BeautifulSoup as bs
import requests as req

def scrape_detik(page):
    global hades
    data = []

    for p in range(1, page + 1):
        print(f"Scraping page {p} ...")
        url = f"https://www.detik.com/search/searchall?query=puan+maharani&siteid=2&sortby=time&page={p}"
        request = req.get(url, hades).text

        soup = bs(request, 'lxml')
        news_list = soup.find('div', class_='list-content')

        if news_list is None:
            print(f"Page {p}: div list-content tidak ditemukan")
            continue

        news_article = news_list.find_all('article')
        for x in news_article:
            # ambil judul
            title_tag = x.find('h3', class_='media__title')
            title = title_tag.text.strip() if title_tag else ""

            # ambil deskripsi
            desc_tag = x.find('div', class_='media__desc')
            desc = desc_tag.text.strip() if desc_tag else ""

            data.append([title, desc])

    return data
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
news=scrape_detik(50)
news_result=pd.DataFrame(news)
news_result.columns = ["Judul", "Konten"]
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
news_result.to_csv('berita.csv',index=False)
news_result.to_excel('berita.xlsx', index=False)
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
import pandas as pd
#read in the data using pandas
url = pd.read_csv('/content/berita.csv')
url.head(50)
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
!pip install Sastrawi
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
import requests as req
from bs4 import BeautifulSoup as bs
import pandas as pd
import re, string

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('punkt_tab')


from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
factory = StemmerFactory()
stemmer = factory.create_stemmer()
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
import re
import string
from bs4 import BeautifulSoup
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from nltk.tokenize import word_tokenize

# Buat stemmer & stopword remover
stemmer = StemmerFactory().create_stemmer()
stopword = StopWordRemoverFactory().create_stop_word_remover()

# Fungsi cleaning
def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()  # huruf kecil
    text = re.sub(r'\d+', '', text)  # hapus angka
    text = text.translate(str.maketrans('', '', string.punctuation))  # hapus tanda baca
    text = re.sub(r'\s+', ' ', text)  # hapus spasi berlebih
    text = BeautifulSoup(text, "html.parser").get_text()  # hapus tag HTML
    return text.strip()

# 1. Cleaning
news_result["clean_text"] = news_result["Konten"].apply(clean_text)

# 2. Hapus stopword
news_result["no_stopword"] = news_result["clean_text"].apply(lambda x: stopword.remove(x))

# 3. Stemming (langsung ke teks penuh)
news_result["news_Stem"] = news_result["no_stopword"].apply(lambda x: stemmer.stem(x))

# 4. Tokenisasi setelah stemming
news_result["news_Token"] = news_result["news_Stem"].apply(lambda x: word_tokenize(x))

# Tampilkan hasil
print(news_result[["Judul","Konten","clean_text","no_stopword","news_Stem","news_Token"]].head())

</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
news_result.to_csv('hasil_preprocessingg_berita.csv')
news_result.to_excel('hasil_preprocessingg_berita.xlsx', index=False)
</code></pre>
      </div>
    </div>

    <!-- Tugas 6 -->
    <div id="tugas6" class="content">
      <h1>TF-IDF & Word Embedding</h1>
      <div class="code-box">
<pre><code class="language-python">
!pip install -q sentence-transformers scikit-learn pandas matplotlib
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
import pandas as pd
import numpy as np

df = pd.read_csv("data.csv", usecols=['title', 'content'])

df['title'] = df['title'].fillna("").astype(str)
df['content'] = df['content'].fillna("").astype(str)

df['text'] = (df['title'].str.strip() + ". " + df['content'].str.strip()).str.strip()

print("Loaded:", len(df))
df.head()
</code></pre>
      </div>

     
       <div class="code-box">
<pre><code class="language-python">
import re

def preprocess(text):
    t = text.lower()
    t = re.sub(r"\s+", " ", t)
    return t.strip()

df['text_clean'] = df['text'].apply(preprocess)

MAX_CHARS = 2000
df['text_trunc'] = df['text_clean'].apply(lambda x: x[:MAX_CHARS])

df[['title','text_trunc']].head()
</code></pre>
      </div>
       <div class="code-box">
<pre><code class="language-python">
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

texts = df['text_trunc'].tolist()

vectorizer = TfidfVectorizer(max_features=8000, ngram_range=(1,2), min_df=2)
X_tfidf = vectorizer.fit_transform(texts)
terms = vectorizer.get_feature_names_out()

print("TF-IDF shape:", X_tfidf.shape)

def find_best_k(X, kmin=2, kmax=6):
    scores = {}
    for k in range(kmin, kmax+1):
        km = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = km.fit_predict(X)
        if len(set(labels)) == 1:
            scores[k] = -1
        else:
            scores[k] = silhouette_score(X, labels)
        print(f"k={k} -> silhouette={scores[k]:.4f}")
    return scores

scores = find_best_k(X_tfidf, 2, 6)

plt.plot(list(scores.keys()), list(scores.values()), marker='o')
plt.xlabel("k")
plt.ylabel("Silhouette")
plt.title("TF-IDF Silhouette Scores")
plt.grid(True)
plt.show()

best_k = max(scores, key=scores.get)
best_k
</code></pre>
      </div>
       <div class="code-box">
<pre><code class="language-python">
k_tfidf = best_k
km_tfidf = KMeans(n_clusters=k_tfidf, random_state=42, n_init=20)
df['cluster_tfidf'] = km_tfidf.fit_predict(X_tfidf)

def top_terms(km, terms, topn=12):
    centers = km.cluster_centers_
    result = {}
    for i, center in enumerate(centers):
        idx = center.argsort()[-topn:][::-1]
        result[i] = [terms[j] for j in idx]
    return result

top_terms_tfidf = top_terms(km_tfidf, terms)

top_terms_tfidf
</code></pre>
      </div>
       <div class="code-box">
<pre><code class="language-python">
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2")

texts_for_embed = df['text_trunc'].tolist()

embeddings = model.encode(
    texts_for_embed,
    batch_size=64,
    show_progress_bar=True,
    convert_to_numpy=True
)

embeddings.shape
</code></pre>
      </div>
       <div class="code-box">
<pre><code class="language-python">
k_embed = k_tfidf  # pakai jumlah cluster sama dengan TF-IDF
km_embed = KMeans(n_clusters=k_embed, random_state=42, n_init=20)
df['cluster_embed'] = km_embed.fit_predict(embeddings)

top_terms_embed = {}
for c in range(k_embed):
    idxs = np.where(df['cluster_embed'] == c)[0]
    if len(idxs) == 0:
        top_terms_embed[c] = []
        continue
    avg = X_tfidf[idxs].mean(axis=0)
    avg = np.asarray(avg).ravel()
    best_idx = avg.argsort()[-12:][::-1]
    top_terms_embed[c] = [terms[i] for i in best_idx]

top_terms_embed

</code></pre>
      </div>
       <div class="code-box">
<pre><code class="language-python">
sil_tfidf = silhouette_score(X_tfidf, df['cluster_tfidf'])
sil_embed = silhouette_score(embeddings, df['cluster_embed'])

sil_tfidf, sil_embed

</code></pre>
      </div>
       <div class="code-box">
<pre><code class="language-python">
def suggest_label(terms):
    if not terms:
        return "unknown"
    return "_".join(terms[:3])

suggest_tfidf = {c: suggest_label(t) for c, t in top_terms_tfidf.items()}
suggest_embed = {c: suggest_label(t) for c, t in top_terms_embed.items()}

df['label_tfidf'] = df['cluster_tfidf'].map(suggest_tfidf)
df['label_embed'] = df['cluster_embed'].map(suggest_embed)

df[['title','content','cluster_tfidf','label_tfidf','cluster_embed','label_embed']].to_csv(
    "data_clustered.csv",
    index=False
)

df.head()

</code></pre>
      </div>
        <div class="code-box">
<pre><code class="language-python">
for c in sorted(df['cluster_tfidf'].unique()):
    print(f"\n=== TF-IDF CLUSTER {c} | {suggest_tfidf[c]} ===")
    samp = df[df['cluster_tfidf']==c].sample(min(5, len(df[df['cluster_tfidf']==c])), random_state=42)
    for t in samp['title']:
        print("-", t[:120])

for c in sorted(df['cluster_embed'].unique()):
    print(f"\n=== EMBED CLUSTER {c} | {suggest_embed[c]} ===")
    samp = df[df['cluster_embed']==c].sample(min(5, len(df[df['cluster_embed']==c])), random_state=42)
    for t in samp['title']:
        print("-", t[:120])
</code></pre>
      </div>
    </div>

    <!-- Tugas 7 -->
    <div id="tugas7" class="content">
      <h1>Crawling Berita Online</h1>
      <div class="code-box">
<pre><code class="language-python">
!pip install gensim
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from gensim import corpora, models
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

nltk.download('stopwords')
nltk.download('wordnet')

</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
df = pd.read_csv("abcnews-date-text.csv")

print("Jumlah data:", len(df))
print(df.head())
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
texts = df['headline_text'].astype(str)
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    tokens = [w for w in text.split() if len(w) > 2 and w not in stop_words]
    tokens = [lemmatizer.lemmatize(w) for w in tokens]
    return tokens

df['tokens'] = texts.apply(preprocess)

print("Contoh hasil tokenisasi:", df['tokens'].iloc[0])
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
dictionary = corpora.Dictionary(df['tokens'])
corpus = [dictionary.doc2bow(text) for text in df['tokens']]
print("Jumlah kata unik di kamus:", len(dictionary))
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
num_topics = 8  # jumlah topik bisa disesuaikan (contoh: 5–10)
lda_model = models.LdaModel(corpus=corpus,
                            id2word=dictionary,
                            num_topics=num_topics,
                            passes=10,
                            random_state=42,
                            chunksize=2000,
                            per_word_topics=True)

print("\n=== Contoh Topik dari LDA ===")
for idx, topic in lda_model.print_topics(num_words=5):
    print(f"Topik {idx}: {topic}")
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
def get_topic_vector(model, bow, num_topics):
    topic_dist = model.get_document_topics(bow, minimum_probability=0)
    topic_vec = [prob for (_, prob) in sorted(topic_dist, key=lambda x: x[0])]
    return topic_vec

X = [get_topic_vector(lda_model, bow, num_topics) for bow in corpus]
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Uji dengan 8 cluster (sama seperti jumlah topik)
kmeans = KMeans(n_clusters=num_topics, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

df['predicted_topic'] = clusters

print("\nContoh hasil klasifikasi topik:")
print(df[['headline_text', 'predicted_topic']].head(10))
</code></pre>
      </div>
    </div>

     <!-- UTS -->
    <div id="uts1" class="content">
      <h1>Klasifikasi Berita</h1>
      <div class="code-box">
<pre><code class="language-python">
import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from scipy.sparse import hstack, csr_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
file_path = "berita_.csv"  
df = pd.read_csv(file_path, engine='python', encoding='utf-8', on_bad_lines='skip')
</code></pre>
      </div>
      
      <div class="code-box">
<pre><code class="language-python">
#membaca data
col_lengths = {col: df[col].astype(str).map(len).median() for col in df.columns}
col_uniques = {col: df[col].nunique(dropna=True) for col in df.columns}
col_info = pd.DataFrame({"median_len": col_lengths, "n_unique": col_uniques}).sort_values("median_len", ascending=False)
text_col = col_info.index[0]
possible_labels = col_info[(col_info.index != text_col) & (col_info['n_unique'] <= 100)].sort_values('n_unique')
label_col = possible_labels.index[0] if not possible_labels.empty else col_info.index[1]
print("Text column:", text_col, "Label column:", label_col)

df = df[[text_col, label_col]].rename(columns={text_col: 'text', label_col: 'label'}).dropna()
df['text'] = df['text'].astype(str)
df['label'] = df['label'].astype(str)
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
fallback_stopwords = {
    "yang","dan","di","ke","untuk","dari","dengan","pada","adalah","itu","ini","saya","kita","kamu","mereka",
    "telah","akan","juga","nya","saja","sebagai","oleh","atau","jika","karena","dapat","lebih","sudah","tidak",
    "masih","tersebut","apa","siapa","kenapa","bagaimana","sebuah","setiap","dalam","hingga","yg","dgn","pd"
}
def preprocess_simple(text):
    text = text.lower()
    text = re.sub(r'http\S+',' ', text)
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    tokens = text.split()
    tokens = [t for t in tokens if t not in fallback_stopwords and len(t) > 2 and not t.isdigit()]
    return " ".join(tokens)

df['clean'] = df['text'].apply(preprocess_simple)
print(df[['text', 'clean']].head())
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
# LDA
count_vectorizer = CountVectorizer(max_df=0.95, min_df=2)
dtm = count_vectorizer.fit_transform(df['clean'])

n_topics = 10
lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, learning_method='batch', max_iter=10)
lda_topics = lda.fit_transform(dtm)

tfidf = TfidfVectorizer(max_df=0.95, min_df=2)
tfidf_features = tfidf.fit_transform(df['clean'])

#mix LDA TF-IDF
lda_sparse = csr_matrix(lda_topics)
X = hstack([tfidf_features, lda_sparse])
y = df['label']
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

nb = MultinomialNB(alpha=1.0)
nb.fit(X_train, y_train)
y_pred_nb = nb.predict(X_test)

svm = LinearSVC(C=1.0, max_iter=10000)
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)

print("Model Naive Bayes dan SVM berhasil dilatih.")

</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
print("Naive Bayes accuracy:", accuracy_score(y_test, y_pred_nb))
print(classification_report(y_test, y_pred_nb))

print("SVM accuracy:", accuracy_score(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm))
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
def get_top_words(model, feature_names, n_top_words=12):
    topics = []
    for topic_idx, topic in enumerate(model.components_):
        top_idx = topic.argsort()[::-1][:n_top_words]
        topics.append([feature_names[i] for i in top_idx])
    return topics

feature_names = count_vectorizer.get_feature_names_out()
top_words = get_top_words(lda, feature_names)
for i, words in enumerate(top_words):
    print(f"Topic {i}: {', '.join(words)}")

</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
# ----------------------------------------------------------
# 7. Evaluasi dan visualisasi hasil
# ----------------------------------------------------------
from sklearn.metrics import accuracy_score

acc_nb = accuracy_score(y_test, y_pred_nb)
acc_svm = accuracy_score(y_test, y_pred_svm)

print("\n=== Evaluasi Model ===")
print(f"Naive Bayes accuracy: {acc_nb:.3f}")
print(f"SVM accuracy: {acc_svm:.3f}")

</code></pre>
      </div>
      
       <div class="code-box">
<pre><code class="language-python">
plt.bar(['Naive Bayes', 'SVM'], [acc_nb, acc_svm], color=['green', 'blue'])
plt.title("Perbandingan Akurasi Model")
plt.ylabel("Akurasi")
plt.ylim(0, 1)
plt.show()
</code></pre>
      </div>
    </div>

     <!-- UTS2 -->
    <div id="uts2" class="content">
      <h1>Analisa Clustering</h1>
      <div class="code-box">
<pre><code class="language-python">
# 1. Import library
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
# 2. Upload file CSV 
from google.colab import files
uploaded = files.upload()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
df = pd.read_csv("spam (1).csv", encoding="latin-1")
df = df[['id', 'Text']]

df.head()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
  df.isnull().sum()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
import re
import string

def preprocess(text):
    text = str(text).lower()                             # lowercase
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)  # remove URL
    text = re.sub(r"\d+", "", text)                      # remove numbers
    text = text.translate(str.maketrans("", "", string.punctuation))  # remove punctuation
    text = re.sub(r"[^a-zA-Z\s]", "", text)              # keep only letters
    text = re.sub(r"\s+", " ", text).strip()             # remove extra spaces
    return text

df['clean_text'] = df['Text'].apply(preprocess)

df[['Text', 'clean_text']].head()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
texts = df['clean_text']

vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(texts)
X.shape
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">

</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
kmeans = KMeans(n_clusters=5, random_state=0)
labels = kmeans.fit_predict(X)

df['cluster'] = labels
df.head()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X.toarray())

plt.figure(figsize=(10,6))
plt.scatter(X_pca[:,0], X_pca[:,1], c=labels)
plt.title("Clustering Dokumen Email (TF-IDF + KMeans)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
for c in sorted(df['cluster'].unique()):
    print(f"\n===== CLUSTER {c} =====\n")
    print(df[df['cluster'] == c]['Text'].head(5))
</code></pre>
      </div>
  </div> 

    <!-- Tugas 8 -->
    <div id="tugas8" class="content">
      <h1>Web Usage Mining</h1>
      <div class="code-box">
<pre><code class="language-python">
import pandas as pd
df = pd.read_csv("data.csv")
df.head()
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
import re

def is_html(url):
    if not isinstance(url, str):
        return False
    url = url.lower()

    # File HTML
    if url.endswith(".html") or url.endswith(".htm"):
        return True

    # URL tanpa ekstensi → dianggap halaman
    # Contoh: /mission, /home, /about
    if re.match(r"^.*/[^/\.]+$", url):
        return True

    return False
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
#filter cuma halaman .HTML 
df_html = df[df['url'].apply(is_html)].reset_index(drop=True)
df_html.head()
</code></pre>
      </div>

      <div class="code-box">
<pre><code class="language-python">
df_html['datetime'] = pd.to_datetime(df_html['time'], unit='s')
df_html = df_html.sort_values(by=['host', 'datetime'])
df_html.head()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
import pandas as pd

TIMEOUT = pd.Timedelta(minutes=30)

def assign_session(group):
    group = group.sort_values("datetime")
    time_diff = group["datetime"].diff()
    session_id = (time_diff > TIMEOUT).cumsum()
    group["session_id"] = session_id
    return group

df_session = df_html.groupby("host").apply(assign_session)
df_session = df_session.reset_index(drop=True)
df_session.head()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
#perjalanan pengguna melalui website.
clickstream = (
    df_session.groupby(["host", "session_id"])["url"]
    .apply(list)
    .reset_index()
)

clickstream.head()

</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
df_session.groupby("host")["session_id"].nunique().describe()

</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
clickstream["len"] = clickstream["url"].apply(len)
clickstream["len"].describe()

</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
!pip install mlxtend
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
#Mencari pola halaman yang sering muncul bersama
atau urutan halaman yang sering terjadi.
!pip install prefixspan

from prefixspan import PrefixSpan

ps = PrefixSpan(clickstream['url'].tolist())
patterns = ps.frequent(10)
patterns[:20]
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
!pip install prefixspan

from prefixspan import PrefixSpan
ps = PrefixSpan(clickstream["url"].tolist())
ps.topk(10)

</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
!pip install plotly

import plotly.graph_objects as go

</code></pre>
      </div>
    </div>

       <!-- Tugas 9 -->
    <div id="tugas9" class="content">
      <h1>Tabel Web Usage Mining</h1>
      <div class="code-box">
<pre><code class="language-python">
import pandas as pd
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
df = pd.read_csv('/content/data.csv')
df.head()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
df = df.sort_values(by=['host', 'time'])
df['time'] = pd.to_numeric(df['time'], errors='coerce')  # memastikan time berupa angka
df.head()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
session_threshold = 1800  # 30 menit

# hitung selisih waktu per user
df['time_diff'] = df.groupby('host')['time'].diff()

# session baru jika selisih waktu > 30 menit
df['new_session'] = (df['time_diff'] > session_threshold).fillna(False).astype(int)

# hitung nomor session (akumulasi)
df['session'] = df.groupby('host')['new_session'].cumsum()

df.head(10)
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
df['session_id'] = df['host'].astype(str) + '_S' + df['session'].astype(str)
df[['host', 'time', 'time_diff', 'session', 'session_id']].head(15)
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
visit_matrix = df.pivot_table(
    index='session_id',
    columns='url',
    values='time',
    aggfunc='count',
    fill_value=0
)

visit_matrix = (visit_matrix > 0).astype(int)
visit_matrix.head()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
print("Jumlah Session:", visit_matrix.shape[0])
print("Jumlah URL unik:", visit_matrix.shape[1])
print("\nTotal URL unik di data asli:", df['url'].nunique())
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
visit_matrix.head()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
visit_matrix['host'] = visit_matrix.index.str.split('_S').str[0]
visit_matrix.head()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
url_cols = visit_matrix.columns.drop('host')
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
import numpy as np

def mode_binary(series):
    counts = series.value_counts()
    if len(counts) == 0:
        return 0
    # Jika sama banyak (misal 1 muncul 2x dan 0 muncul 2x), pilih 1 (atau bisa diubah ke 0 jika mau)
    if len(counts) > 1 and counts.iloc[0] == counts.iloc[1]:
        return 1
    return counts.idxmax()

user_summary = visit_matrix.groupby('host')[url_cols].agg(mode_binary)
user_summary.head()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
print("Jumlah user:", user_summary.shape[0])
print("Jumlah kolom (URL):", user_summary.shape[1])
user_summary.head()
</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
user_summary.to_excel('/content/user_summary_web_usage.xlsx')
print("File sukses dibuat!")
</code></pre>
      </div>
    </div> 

     <!-- Tugas 10 -->
    <div id="tugas10" class="content">
      <h1>Revisi TF-IDF & Word Embedding</h1>
      <div class="code-box">
<pre><code class="language-python">
!pip install pandas scikit-learn nltk gensim

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from gensim.models import Word2Vec
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re

# download NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# load dataset
df = pd.read_csv('berita_cnn.csv')

# cek kolom
print(df.columns)
# kita pakai kolom 'content' sebagai teks
df = df.rename(columns={'content':'text'})
print(df[['text']].head())

</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
import re
from nltk.corpus import stopwords
import pandas as pd

# pastikan stopwords sudah didownload
import nltk
nltk.download('stopwords')

stop_words = set(stopwords.words('indonesian'))

def preprocess_text_simple(text):
    if pd.isna(text):
        return ''
    text = text.lower()
    text = re.sub(r'\d+', '', text)  # hapus angka
    text = re.sub(r'[^\w\s]', '', text)  # hapus tanda baca
    tokens = text.split()  # pakai split sederhana, ganti word_tokenize
    tokens = [w for w in tokens if w not in stop_words]
    return ' '.join(tokens)

df['clean_text'] = df['text'].apply(preprocess_text_simple)
df['clean_text'].head()

</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
# TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=8000)
tfidf_matrix = tfidf_vectorizer.fit_transform(df['clean_text'])
print("TF-IDF shape:", tfidf_matrix.shape)

# Tentukan jumlah cluster dengan silhouette score
sil_scores = {}
for k in range(2, 7):
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(tfidf_matrix)
    score = silhouette_score(tfidf_matrix, labels)
    sil_scores[k] = score
    print(f"k={k} -> silhouette={score:.4f}")

# pilih k terbaik (nilai silhouette tertinggi)
best_k = max(sil_scores, key=sil_scores.get)
print("Best k:", best_k)

# final clustering
kmeans_tfidf = KMeans(n_clusters=best_k, random_state=42)
df['tfidf_cluster'] = kmeans_tfidf.fit_predict(tfidf_matrix)

</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
# tokenisasi
df['tokens'] = df['clean_text'].apply(lambda x: x.split())

# training Word2Vec
w2v_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=2, workers=4)
w2v_model.save("berita_cnn_word2vec.model")

# representasi dokumen = rata-rata embedding kata
def get_doc_vector(tokens):
    vecs = [w2v_model.wv[w] for w in tokens if w in w2v_model.wv]
    if len(vecs)==0:
        return np.zeros(100)
    return np.mean(vecs, axis=0)

doc_vectors = np.array(df['tokens'].apply(get_doc_vector).tolist())

# clustering
kmeans_embed = KMeans(n_clusters=best_k, random_state=42)
df['embed_cluster'] = kmeans_embed.fit_predict(doc_vectors)

</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
top_n = 12
tfidf_terms = tfidf_vectorizer.get_feature_names_out()
tfidf_clusters = {}
for i in range(best_k):
    cluster_idx = np.where(df['tfidf_cluster']==i)[0]
    cluster_tfidf = tfidf_matrix[cluster_idx].mean(axis=0)
    top_terms_idx = np.argsort(cluster_tfidf.A1)[::-1][:top_n]
    top_terms = [tfidf_terms[j] for j in top_terms_idx]
    tfidf_clusters[i] = top_terms
    print(f"Cluster {i} top terms:", top_terms)

</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
for i in range(best_k):
    print(f"\n=== TF-IDF CLUSTER {i} | {'_'.join(tfidf_clusters[i][:3])} ===")
    cluster_headlines = df[df['tfidf_cluster']==i]['title'].head(5).tolist()
    for h in cluster_headlines:
        print("-", h)

</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
embed_clusters = {}
for i in range(best_k):
    cluster_idx = df[df['embed_cluster']==i].index
    words = [w for tokens in df.loc[cluster_idx,'tokens'] for w in tokens if w in w2v_model.wv]
    freq = pd.Series(words).value_counts()
    top_words = freq.head(top_n).index.tolist()
    embed_clusters[i] = top_words
    print(f"Embed Cluster {i} top words:", top_words)

</code></pre>
      </div>
      <div class="code-box">
<pre><code class="language-python">
for i in range(best_k):
    print(f"\n=== EMBED CLUSTER {i} | {'_'.join(embed_clusters[i][:3])} ===")
    cluster_headlines = df[df['embed_cluster']==i]['title'].head(5).tolist()
    for h in cluster_headlines:
        print("-", h)

</code></pre>
      </div>
    </div>

         <!-- Tugas 11 -->
    <div id="tugas11" class="content">
      <h1>Graph Facebook</h1>
      <div class="code-box">
<pre><code class="language-python">
!pip install python-louvain
</code></pre>
      </div>
       <div class="code-box">
<pre><code class="language-python">
import networkx as nx
import matplotlib.pyplot as plt
import community.community_louvain as community_louvain

</code></pre>
      </div>
       <div class="code-box">
<pre><code class="language-python">
file_path = "/content/0.edges"  

G = nx.read_edgelist(file_path, nodetype=int)

print("Total Node:", G.number_of_nodes())
print("Total Edge:", G.number_of_edges())

</code></pre>
      </div>
       <div class="code-box">
<pre><code class="language-python">
partition = community_louvain.best_partition(G)
num_communities = len(set(partition.values()))

print("Total Komunitas Terdeteksi:", num_communities)

</code></pre>
      </div>
       <div class="code-box">
<pre><code class="language-python">
pos = nx.spring_layout(G, k=0.15, iterations=50)

color_map = [partition[n] for n in G.nodes()]
sizes = [80 + G.degree(n)*3 for n in G.nodes()]

plt.figure(figsize=(16,14))
nx.draw(
    G, pos,
    node_color=color_map,
    cmap=plt.cm.tab20,
    node_size=sizes,
    alpha=0.85,
    edge_color="gray",
    linewidths=0.3,
    with_labels=False
)

plt.title("Deteksi Komunitas Facebook SNAP (Louvain)", fontsize=20)
plt.axis("off")
plt.show()

</code></pre>
      </div>
       <div class="code-box">
<pre><code class="language-python">
from collections import Counter

community_count = Counter(partition.values())
community_count
#(komunitas :anggota(node))
</code></pre>
      </div>
       <div class="code-box">
<pre><code class="language-python">
deg = dict(G.degree())
sorted_deg = sorted(deg.items(), key=lambda x: x[1], reverse=True)
sorted_deg[:10]
#(node ,degree(hubungan) )

</code></pre>
      </div>
    </div>

    <!-- Tugas 11 -->
    <div id="tugas11" class="content">
      <h1>Word Graph</h1>
      <div class="code-box">
<pre><code class="language-python">
!pip install pymupdf nltk networkx pandas numpy matplotlib
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
import fitz  # PyMuPDF

file_path = '/content/SISTEM PENDUKUNG KEPUTUSAN.PDF'

# Buka PDF
doc = fitz.open(file_path)

# Ekstrak teks
text = ""
for page in doc:
    text += page.get_text()

# Simpan ke file txt
with open("output.txt", "w", encoding="utf-8") as f:
    f.write(text)

# Tampilkan 500 karakter pertama
print(text[:500])
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')  # TAMBAHKAN INI
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
from nltk.tokenize import sent_tokenize, word_tokenize

with open("output.txt", "r", encoding="utf-8") as f:
    teks = f.read()

sentences = sent_tokenize(teks)
print(sentences[:5])
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
from nltk.corpus import stopwords
nltk.download("stopwords")

stop_words = set(stopwords.words('indonesian'))

words = word_tokenize(teks.lower())
words = [w for w in words if w.isalnum() and w not in stop_words]

print(words[:20])
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
#co-occurance
from collections import defaultdict, Counter
import numpy as np
import pandas as pd

window_size = 2
co_occurrences = defaultdict(Counter)

for i, word in enumerate(words):
    for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):
        if i != j:
            co_occurrences[word][words[j]] += 1

unique_words = list(set(words))
word_index = {word: idx for idx, word in enumerate(unique_words)}

co_matrix = np.zeros((len(unique_words), len(unique_words)), dtype=int)

for word, neighbors in co_occurrences.items():
    for neighbor, count in neighbors.items():
        co_matrix[word_index[word]][word_index[neighbor]] = count

co_df = pd.DataFrame(co_matrix, index=unique_words, columns=unique_words)
co_df.head()
</code></pre>
      </div>

       <div class="code-box">
<pre><code class="language-python">
import networkx as nx
import matplotlib.pyplot as plt

# Buat graph dari co-occurrence matrix
G = nx.from_numpy_array(co_matrix)

# ===== STATISTIK =====
jumlah_kalimat = len(sentences)
jumlah_kata_total = len(words)
jumlah_kata_unik = len(unique_words)
jumlah_node = G.number_of_nodes()
jumlah_edge = G.number_of_edges()

print("===== STATISTIK TEKS & GRAPH =====")
print("Jumlah kalimat          :", jumlah_kalimat)
print("Jumlah kata total       :", jumlah_kata_total)
print("Jumlah kata unik        :", jumlah_kata_unik)
print("Jumlah node dalam graph :", jumlah_node)
print("Jumlah edge dalam graph :", jumlah_edge)

# ===== VISUALISASI GRAPH =====
plt.figure(figsize=(10,8))
nx.draw(G, node_size=100, with_labels=False)
plt.title("Word Graph dari Paper + Statistik")
plt.show()
</code></pre>
      </div>
            <div class="code-box">
<pre><code class="language-python">
pagerank = nx.pagerank(G)

# Urutkan kata berdasarkan PageRank tertinggi
sorted_pr = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)

# Tampilkan 20 kata teratas
for i in range(20):
    idx = sorted_pr[i][0]
    kata = unique_words[idx]
    skor = sorted_pr[i][1]
    print(f"{kata} : {skor}")
</code></pre>
      </div>
    </div>

     <!-- Tugas 12 -->
    <div id="tugas12" class="content">
      <h1>Load Paper</h1>
      <div class="code-box">
<pre><code class="language-python">
#APP.py
  import streamlit as st
import matplotlib.pyplot as plt
import pandas as pd
import networkx as nx
from nltk.tokenize import sent_tokenize

from utils import (
    extract_text_from_pdf,
    preprocess_text,
    build_cooccurrence,
    build_graph,
    centrality_analysis
)

st.set_page_config(page_title="Word Graph & Centrality", layout="wide")

st.title("📄 Word Graph & Centrality Analysis")

uploaded_file = st.file_uploader("Upload Paper (PDF)", type=["pdf"])

# ======================================================
# SEMUA PROSES HARUS DI DALAM BLOK INI
# ======================================================
if uploaded_file is not None:

    # 1. Ekstraksi teks
    text = extract_text_from_pdf(uploaded_file)

    # 2. Tokenisasi kalimat (untuk statistik)
    sentences = sent_tokenize(text)

    # 3. Preprocessing kata
    words = preprocess_text(text)

    # Proteksi kalau teks terlalu pendek
    if len(words) < 10:
        st.warning("Dokumen terlalu pendek untuk dianalisis.")
        st.stop()

    # 4. Co-occurrence matrix
    matrix, unique_words = build_cooccurrence(words)

    # 5. GRAPH (INI YANG HILANG DI KODEMU)
    G = build_graph(matrix)

    # =========================
    # STATISTIK
    # =========================
    st.subheader("📊 Statistik Teks & Graph")

    col1, col2 = st.columns(2)

    with col1:
        st.metric("Jumlah Kalimat", len(sentences))
        st.metric("Jumlah Kata Total", len(words))
        st.metric("Jumlah Kata Unik", len(unique_words))

    with col2:
        st.metric("Jumlah Node", G.number_of_nodes())
        st.metric("Jumlah Edge", G.number_of_edges())

    # =========================
    # WORD GRAPH
    # =========================
    st.subheader("📌 Word Graph")

    plt.figure(figsize=(12, 10))
    pos = nx.spring_layout(G, k=1.5, seed=42)
    nx.draw(G, pos, node_size=150, with_labels=False)
    st.pyplot(plt)

    # =========================
    # CENTRALITY ANALYSIS
    # =========================
    st.subheader("📊 Centrality Analysis")

    centralities = centrality_analysis(G)

    for name, values in centralities.items():
        st.write(f"### {name}")

        df = pd.DataFrame(
            [(node, unique_words[node], score) for node, score in values.items()],
            columns=["Node", "Kata", "Score"]
        )

        df = df.sort_values("Score", ascending=False)
        st.dataframe(df.head(10))

</code></pre>
      </div>
       <div class="code-box">
<pre><code class="language-python">
#UTILS.PY
  import fitz
import nltk
import re
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import defaultdict, Counter

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

def extract_text_from_pdf(pdf_file):
    doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
    text = ""
    for page in doc:
        text += page.get_text()
    return text

def preprocess_text(text):
    words = word_tokenize(text.lower())
    words = [
        w for w in words
        if re.match(r'^[a-zA-Z]+$', w)
        and w not in stopwords.words('indonesian')
    ]
    return words

def build_cooccurrence(words, window_size=2):
    co_occurrences = defaultdict(Counter)

    for i, word in enumerate(words):
        for j in range(max(0, i-window_size), min(len(words), i+window_size+1)):
            if i != j:
                co_occurrences[word][words[j]] += 1

    unique_words = list(set(words))
    word_index = {w: i for i, w in enumerate(unique_words)}

    matrix = np.zeros((len(unique_words), len(unique_words)), dtype=int)

    for word, neighbors in co_occurrences.items():
        for neighbor, count in neighbors.items():
            matrix[word_index[word]][word_index[neighbor]] = count

    return matrix, unique_words

def build_graph(matrix):
    return nx.from_numpy_array(matrix)

def centrality_analysis(G):
    return {
        "PageRank": nx.pagerank(G),
        "Degree Centrality": nx.degree_centrality(G),
        "Betweenness Centrality": nx.betweenness_centrality(G)
    }

def draw_graph(G, title, st):
    plt.figure(figsize=(12,10))
    pos = nx.spring_layout(G, k=1.5, seed=42)
    nx.draw(G, pos, with_labels=True, node_size=2000)
    plt.title(title)
    st.pyplot(plt)

</code></pre>
      </div>
    </div>
</div>

  <!-- Script untuk switch konten -->
  <script>
    function showContent(id) {
      const contents = document.querySelectorAll('.content');
      contents.forEach(c => c.classList.remove('active'));
      document.getElementById(id).classList.add('active');
    }
  </script>
</body>

</html>














